{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: Evaluation\n",
    "\n",
    "As important as creating a good representation to the problem and creating a good model, knowing how to evaluate if it solves the problem is a fundamental, especially in the security area. It is normal to see in the literature works that try to reach an accuracy near 100% in malware classificatino problems, i.e., they evaluate only if all the samples, regardless their classes and data distribution, were correctly classified, which may be not be the best way to evaluate the problem [Ceschin et al. 2018]. For example, consider that a model is evaluated using ten samples: eight of them are benign and two, malign. This model has an accuracy of 80%. Is 80% a good accuracy? Assuming that the model classifies correctly only the eight benign samples, it is not capable of identifying any malign sample and yet it has an accuracy of 80%, giving the false impression that the model works significantly well. We need to take into account that, in binary problems in general, a machine learning model must be robust enough to identify patterns in a generic way, regardless its class, given that we do not want a model that classifies everything as being from an unique class. In this section we are going to present the most common metrics used in security problems, as well as method to validate the solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, we are going to extract features from Brazilian Malware dataset again, as in previous section. First, we read the CSV using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# dataset location\n",
    "data_path = \"./datasets/brazilian-malware.csv\"\n",
    "# read CSV dataset\n",
    "data = pd.read_csv(data_path, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we list all the features of the dataset, categorizing them into numerical (real and integer numbers) and textual (libraries, system calls, compilers, etc). We also get the label and select the columns to be ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical attributes\n",
    "NUMERICAL_ATTRIBUTES = ['BaseOfCode', 'BaseOfData', 'Characteristics', 'DllCharacteristics', \n",
    "                      'Entropy', 'FileAlignment', 'ImageBase', 'Machine', 'Magic',\n",
    "                      'NumberOfRvaAndSizes', 'NumberOfSections', 'NumberOfSymbols', 'PE_TYPE',\n",
    "                      'PointerToSymbolTable', 'Size', 'SizeOfCode', 'SizeOfHeaders',\n",
    "                      'SizeOfImage', 'SizeOfInitializedData', 'SizeOfOptionalHeader',\n",
    "                      'SizeOfUninitializedData']\n",
    "\n",
    "# textual attributes\n",
    "TEXTUAL_ATTRIBUTES = ['Identify', 'ImportedDlls', 'ImportedSymbols']\n",
    "\n",
    "# label used to classify\n",
    "LABEL = 'Label'\n",
    "\n",
    "# attributes that are not used\n",
    "UNUSED_ATTRIBUTES = ['FirstSeenDate', 'SHA1', 'TimeDateStamp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we get the labels and remove unused attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = data[LABEL].values\n",
    "# remove unused attributes and label\n",
    "for a in UNUSED_ATTRIBUTES:\n",
    "    del data[a]\n",
    "del data[LABEL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we split the dataset in half (the first one is for the training set and the last, for the testing set):TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in half\n",
    "def split_data(data):\n",
    "    # get mid of data\n",
    "    mid = int((len(data) + 1)/2)\n",
    "    # split data into train and test\n",
    "    train_data = data[:mid]\n",
    "    test_data = data[mid:]\n",
    "    # return train and test data\n",
    "    return(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, _ = split_data(data)\n",
    "# label, _ = split_data(label)\n",
    "train_data, test_data = split_data(data)\n",
    "train_label, test_label = split_data(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we created a method to extract features, given a train and test set and a feature extractor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# extract features from textual attributes\n",
    "def textual_feature_extraction(train_data, test_data, extractor=TfidfVectorizer(max_features=100)):\n",
    "    vectorizer = extractor\n",
    "    # train vectorizer\n",
    "    vectorizer.fit(train_data)\n",
    "    # transform train and test data to features\n",
    "    train_features = vectorizer.transform(train_data)\n",
    "    test_features = vectorizer.transform(test_data)\n",
    "    # return train and test features\n",
    "    return(train_features, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain numerical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_data[NUMERICAL_ATTRIBUTES].values\n",
    "test_features = test_data[NUMERICAL_ATTRIBUTES].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25091, 21), (25090, 21))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape, test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain textual attributes and append to features array already initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# extract features from each textual attribute\n",
    "for a in TEXTUAL_ATTRIBUTES:\n",
    "    # extract features from current attribute\n",
    "    train_texts, test_texts = textual_feature_extraction(train_data[a], test_data[a])\n",
    "    train_features = np.concatenate((train_features, train_texts.toarray()), axis=1)\n",
    "    test_features = np.concatenate((test_features, test_texts.toarray()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25091, 321), (25090, 321))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape, test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we created a normalization method, which normalizes both training and testing set, given a scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def normalization(train_features, test_features, scaler=MinMaxScaler()):\n",
    "    # train minmax\n",
    "    scaler.fit(train_features)\n",
    "    # transform features\n",
    "    train_features_norm = scaler.transform(train_features)\n",
    "    test_features_norm = scaler.transform(test_features)\n",
    "    # return normalized train and test features\n",
    "    return(train_features_norm, test_features_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_norm, test_features_norm = normalization(train_features, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train a Random Forest classifier and predict the test labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25090,) (25090,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# initialize classifier\n",
    "clf = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "# train classifier\n",
    "clf.fit(train_features_norm, train_label)\n",
    "# predict test classes\n",
    "test_pred = clf.predict(test_features_norm)\n",
    "# print test pred and real labels shape\n",
    "print(test_pred.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now on we have the real labels (*test_label*) and the predicted ones (test_pred) of the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "To correctly evaluate a model, we need to choose the right metrics, given that each one can present you a different perspective of the problem (and yet give you false impressions about it, as in previous example). In this course we are going to present accuracy, confusion matrix, recall, precision and f1score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "Accuracy measures the percentage of samples that the model correctly classified from a certain set (usually, the testing set). Basically, it corresponds to the number of samples correctly classified divided by the total number of samples presented. Scikit-Learn implements this metric through the method [*accuracy_score*](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html), passing as parameters the predictions made by the classifier and the real labels of the set used, as shown in the code below. This metric is not recommended for problems that uses imbalanced datasets, given that it can have a high value, even when the classifier favors the majority class and misses all the samples from the minority classes [Gron 2017]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8836986847349542\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_label, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "The confusion matrix is a way to better visualize the results generated by a classifier. Given $C$, a confusion matrix, $C_{i,j}$ is equal to the number of observations that the classifier considered a sample from class $i$ as being from class $j$. In binary classification we can extract the following informations from the confusion matrix [Pedregosa et al. 2011]:\n",
    "\n",
    "* **True Negatives (TN):** $C_{0,0}$.\n",
    "* **False Negatives (FN):** $C_{1,0}$.\n",
    "* **True Positives (TP):** $C_{1,1}$.\n",
    "* **False Positives (FP):** $C_{0,1}$.\n",
    "\n",
    "The code below presents an example of confusion matrix, which is computed using [the method *confusion_matrix* from Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html), passing as parameters the predictions made by the classifier and the real labels of the set used. It is possible to observe that the number of True Negatives (TN) is $6987$ ($C_{0,0}$), False Negatives (FN) is $2672$ ($C_{1,0}$), True Positives (TP) is $15185$ ($C_{1,1}$) and the False Positives (FP) is only $46$ ($C_{0,1}$). Through this information, two new measures can be extracted (recall and precision), generating a new metric that uses both (f1score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6987    46]\n",
      " [ 2872 15185]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(test_label, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "Recall, also called sensitivity or True Positive Rate (TPR), is the proportion of positive instances that are correctly classified by the model, i.e., the ability of the classifier to find all the positive samples [Gron 2017, Pedregosa et al. 2011]. It uses the number of true positives (TP) and false negatives (FN) and is given by the equation $Recall = \\frac{TP}{TP+FN}$. Scikit-Learn implements this metric through the method [*recall_score*](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html), passing as parameters the predictions made by the classifier and the real labels of the set used, as shown in code below. In our sample, we obtained around 84% of recall, indicating that the model considers that some malware are goodware (around 16%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8409481087666832\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "print(recall_score(test_label, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "Precision is a metric capable of measuring the accuracy of the positive predictions of the classifier, i.e., it measure the ability of the classifier not labeling as positive a sample that is negative [Gron 2017, Pedregosa et al. 2011]. It uses the number of true positives (TP) and the number of false positives (FP), as show by the equation $Precision = \\frac{TP}{TP+FP}$. Scikit-Learn also implements this metric through the method [*precision_score*](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html), passing as parameters the predictions made by the classifier and the real labels of the set used, as shown in code below. In our sample, we obtained around 99% of precision, indicating that few goodware are being considered as being malware (less than 1%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9969798437397414\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "print(precision_score(test_label, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-Score\n",
    "\n",
    "F1-Score uses both recall and precision to create an unique metric, which is the harmonic average between them, given by the equation $\\textit{F1Score} = \\frac{2*(Precision*Recall)}{Precision + Recall}$. Scikit-Learn implements this metric through the method [*f1_score*](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html), passing as parameters the predictions made by the classifier and the real labels of the set used, as shown in code below. In our problem, we got around 91% of f1score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9123407834655132\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(test_label, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although interesting, it is always recommended to report recall and precision individually, once that in given problems, one can be more important than other and f1score may not show this necessity [Gron 2017]. For a malware detector, for example, it may be better that a malware is not detected than blocking a benign software (high precision)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Thresholds\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8076524511757672\n",
      "Recall: 0.7345073932546935\n",
      "Precision: 0.9975930801053028\n",
      "F1Score: 0.8460704261291146\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.7\n",
    "test_pred_proba = clf.predict_proba(test_features_norm)\n",
    "test_pred = (test_pred_proba[:,1] > threshold).astype('int')\n",
    "print(\"Accuracy:\",accuracy_score(test_label, test_pred))\n",
    "print(\"Recall:\", recall_score(test_label, test_pred))\n",
    "print(\"Precision:\", precision_score(test_label, test_pred))\n",
    "print(\"F1Score:\", f1_score(test_label, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = []\n",
    "precision = []\n",
    "thresholds = []\n",
    "for threshold in np.arange(0,1.01,0.025):\n",
    "    test_pred_proba = clf.predict_proba(test_features_norm)\n",
    "    test_pred = (test_pred_proba[:,1] >= threshold).astype('int')\n",
    "    thresholds.append(threshold)\n",
    "    recall.append(recall_score(test_label, test_pred))\n",
    "    precision.append(precision_score(test_label, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(thresholds, recall, label=\"Recall\", color=\"goldenrod\")\n",
    "plt.plot(thresholds, precision, label=\"Precision\", color=\"green\")\n",
    "plt.xticks(np.arange(0,1.01,0.1))\n",
    "plt.yticks(np.arange(0.5,1.01,0.05))\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Recall e Precision\")\n",
    "plt.title(\"Recall e Precision X Threshold\")\n",
    "plt.savefig(\"rp_graph.png\", dpi=300)\n",
    "plt.savefig(\"rp_graph.pdf\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57609562 0.92430279 0.99561753 0.99402152 0.99481865 0.98326026\n",
      " 0.9920287  0.9039458  0.98963317 0.98405104]\n",
      "0.9337775077032392\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "# initialize classifier\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "# get results\n",
    "results = cross_val_score(clf, train_features_norm, train_label, cv=10, scoring=\"accuracy\")\n",
    "# print accuracy per fold\n",
    "print(results)\n",
    "# print mean of accuracy\n",
    "print(np.mean(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Ceschin et al. 2018] Ceschin, F., Pinage, F., Castilho, M., Menotti, D., Oliveira, L. S., and Gregio, A. (2018). The need for speed: An analysis of brazilian malware classifers. IEEE Security Privacy, 16(6):31–41.\n",
    "\n",
    "[Pedregosa et al. 2011] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. (2011). Scikitlearn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**<< Previous Section**](05_models.ipynb) | **Next Section >>**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
