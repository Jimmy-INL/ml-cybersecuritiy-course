{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Models\n",
    "\n",
    "In this section we present the most used machine learning models in problems involving security, including their theory, algorithms and codes of how to use them. Among the models, there are classifiers, detectors and clustering techniques, all of them explained here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Classifiers\n",
    "\n",
    "Classifiers aim to classify a given input sample into a previously known class by them during the training. The training is the step that the classifier learns the patterns of each class with the data presented to it (together with their labels), adapting its parameters to the problem. This type of problem is known as supervised learning [Bishop 2006]. After the training, the model can be used to classify any unknown data, allowing it to be effectively used. Here we present the following classifiers: K-Nearest Neighbors, based in neighborhood, Random Forest, an ensemble based in decision trees, Support Vector Machines, based in the construction of a optimal hyperplane, and Multi-Layer Perceptron, a neural network type used a lot in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN)\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a machine learning model based in distance, which classification of a new instance is based on the distance of the $k$ training samples most closer to a given testing sample. Thus, a new unknown sample will be classified as being from the most prevalent classes between these $k$ samples, as shown in figure below [Michie et al. 1994], where a new instance will be classified as red when $k=3$, green, when $k=5$ and unknown when $k=6$ (the result is a draw, that is why even numbers are not recommended in binary classification - when there are just two classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/knn1.png\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, the distance used by KNN is the Euclidean distance. Given an instance $x$, described by $(a_{1}(x),a_{2}(x),...,a_{n}(x))$, where $a_{i}(x)$ is the $i$-th feature, the distance between two instances $x_{i}$ and $x_{j}$ is defined by the equation $ d(x_{i},x_{j}) = \\sqrt{ \\sum \\limits_{r=1}^{n} (a_{r}(x_{i})-a_{r}(x_{j}))^{2}) } $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Random forest is a classifier that consists of a collection (ensemble) of tree-structured classifiers, each of them trained on bagged data using random selection of features (bootstrap aggregating or bagging) and cast an vote for the most popular class for a given input [Breiman 2001]. The figure below shows an example of a random forest, containing $L$ trees using bagging. To better understand this ensemble. we will explain how a decision tree works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/randomforest1.png\" width=\"500\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is basically a classifier that creates a set of if-then rules to classify new samples. These rules improve human readability, since it makes easy to understand the model and which attributes are important. The classification of new instances is done by sorting them down the tree from the root to some leaf node, which provides the classification of the instance. Each node of the tree represents a test of some attribute of the instance, and each branch descending from that node corresponds to one of the possible values for this attribute [Mitchell 1997]. Figure below shows an example of a decision tree used to classify if a given moment of a day is good do play tennis (based on weather conditions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/randomforest2.png\" width=\"500\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training of a decision tree is based in two measures: entropy and information gain. The entropy, represented by the equation $Entropy(S) = \\sum_{i=1}^{c} - p_{i}\\log_{2}(p_{i})$, where $S$ is a set of training samples and $c$, the number of classes, measures the homogeneity of the samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information gain, as shown in equation $Gain(S,A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_{v}|}{|S|} Entropy(S_{v})$, is used as an attribute selection measure. The tree is built according to the information gain of the attribute $A$, since you always pick the attribute with higher information gain as splitting attribute, to create new branches for each value of this attribute (or intervals, in the case of numerical features) [Mitchell 1997]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm for training a decision tree (the basic algorithm, called ID3), given a set of *examples* (training samples), the *target attribute* (label, the attribute to be predicted\n",
    "by the tree) and the *attributes* (list of the attributes of the samples, excluding the label), is the following [Mitchell 1997]:\n",
    "\n",
    "1. Create a root node for the tree.\n",
    "2. If all *examples* are positive, return the single-node tree root, with $label=+$.\n",
    "3. If all *examples* are negative, return the single-node tree root, with $label=-$.\n",
    "4. If *attributes* is empty, return the single-node tree root, with $label=$ most common value of *target attributes* in *examples*.\n",
    "5. Otherwise, do the following:\n",
    "   1. $A=$ the attribute from *attributes* that best  classifies *examples*. The best attribute is the one with highest information gain.\n",
    "   2. The decision attribute for root is $A$.\n",
    "   3. For each possible value $v_{i}$ of $A$:\n",
    "      1. Add a new tree branch below root, corresponding to the test $A=v_{i}$.\n",
    "      2. Let $examples_{v_{i}}$ be the subset of  *examples* that have value $v_{i}$ for $A$:\n",
    "         1. If $examples_{v_{i}}$ is empty, them, below this new branch, add a leaf node with $label=$ most common value of *target attributes* in *examples*.\n",
    "         2. Else, below this new branch, create a new subtree, going back to step 1 using a subset of *examples* ($examples_{v_{i}}$) and *attributes* ($attributes-\\{A\\}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "\n",
    "The Support Vector Machine (SVM), originaly developed for binary classification, finds, in linearly separable problems, the construction of a hyperplane as a decision surface (a boundary), such that the separation between the samples is maximal. When the patterns are non-linearly separable, the SVM finds a mapping function, which projects the data in a space where the data is linearly separable. The main idea of this classifier is to maximize the hyperplane margin from the training data. An optimal hyperplane is the one whose margin distance to the positive class is the same margin distance to the negative class. The figure below illustrates an optimal hyperplane defined by the support vectors, the training samples  most closer to it [Cortes and Vapnik 1995, Fradkin and Muchnik 2006]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/svm1.png\" width=\"500\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most of times, the problems are not linearly separable. Due to that, it's necessary to project the data in a space where they are linearly separable, called feature space. The kernel function is responsible for this projection and this process is called kernel trick. After projected, it's possible to find a hyperplane that separates the data in that space. The figure below exemplifies the use of the kernel trick to project the data in another dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/svm2.png\" width=\"800\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision function responsible for building the hyperplane is defined by equation $f(x) = \\sum\\limits_{i}\\alpha_{i}y_{i}K(x,x_{i})+b$, where $K$ is \n",
    "the kernel function, $\\alpha$ and $b$ are parameters found during the training, $x_{i}$ are the feature vectors and $y_{i}$, their labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the majority of the real problems involve more than two classes and the SVM is a binary classifier, it's necessary to use a different decision approach. The most common is the one\n",
    "versus all, also called one versus the rest [Manning et al. 2008b]. In this approach, there are $q$ classifiers (SVMs), where $q$ is the number of classes. Each SVM $c_{i}$ is trained to the class $i$, using as counterexample the samples from another classes. The final decision can be made through a \"vote counting\" [Milgram et al. 2006]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron (MLP)\n",
    "\n",
    "A neural network is a computational model of a human brain that can be composed of hundreds or thousands of neurons (processing units). In general, it's a machine that is designed to model the way in which the brain performs a particular task. The smallest component of a neural network is a perceptron, a processing unit that simulates a neuron [Haykin 2009]. Figure below shows the structure of a perceptron, where $x=\\{x_{1}, x_{2}, ..., x_{n}\\}$ are the signals (input), $w=\\{w_{1}, w_{2}, ..., w_{n}\\}$ are the weights, $\\varphi(.)$ is the activation function and $y$ is the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/mlp1.png\" width=\"400\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation $y = \\varphi (\\sum_{i=1}^{n} w_{i} \\times x_{i} + b)$ presents the output ($y$) of a perceptron. The input signals are multiplied to the weights, that are adapted on an iteration-by-iteration basis, using an error-correction rule known as the perceptron convergence algorithm. It also includes an bias ($b$), which has the effect of increasing or lowering the net input of the activation function. The activation function is responsible for determining the shape and intensity of the values transmitted from one neuron to another (or to the output), limiting the amplitude of the output [Haykin 2009]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training of a single perceptron helps to understand better the operation of a neural network. The algorithm is the following [Haykin 2009]:\n",
    "\n",
    "1. Initialize weights and bias with small random values.\n",
    "2. Apply current sample input pattern and check the network output ($y$).\n",
    "3. Calculate output error ($e$), comparing it (the output $y$) to the expected value ($t_{j}$), as shown in equation $e = t_{j} - y$.\n",
    "4. If the output error is equal zero ($e=0$), it means that the output is correct. In that case, a new sample is presented, going back to the step 2. \n",
    "5. Otherwise, if the output error is different from zero ($e \\neq 0$), it's necessary to update the weights and the bias, as shown in equations $w_{j}=w_{j}' + e \\times x_{j}$ and $b = b' + e$, respectively.\n",
    "6. Go back to step 2 and present a new sample to the network. The stopping criteria can be based in iterations number, average error rate, accuracy, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite of being efficient, a single perceptron can only solve linearly separable problems, which, in most of times, are not present in the real world, i.e., the presence of non-linear \n",
    "problems is very common in real problems. Due to that, neural networks usually combine more than one neuron, which makes it possible to them separate non-linear problems [Haykin 2009]. Figure below shows two examples of problems, the first one (left) linear and the second (right), non-linear. As the decision boundary of the perceptron is defined by a line, it solves problems like the first one. A more complex network, such as multilayer perceptron, composed of multiple neurons, can solve the second one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/mlp2.png\" width=\"800\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multilayer perceptron (MLP) neural network is composed by source nodes, that are the input, one layer or more of perceptrons, called hidden layers, and the output neurons. All the layers, except the input, are composed by neurons, each one of them initialized with random weights and bias. This type of network is progressive, i.e., the neurons of a certain layer\n",
    "are just connected to the next layer. Thus, the input passes through all existing layers. The number of input nodes is the dimensionality of the input data and the number of neurons \n",
    "in the output is generally composed by the number of classes of the problem (in this case, each neuron represents one class, i.e., the output value of the neuron is directly related\n",
    "to its respective class. The higher the value, the higher the chances of that sample being of that class) [Haykin 2009]. The figure below presents a multilayer perceptron neural network with two hidden layers and three output neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/mlp3.png\" width=\"800\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important algorithm for neural networks in general is back propagation, a computationally efficient method for the training of multilayer perceptrons. Briefly, this algorithm implements gradient descent in weight space for a \n",
    "multilayer perceptron, efficiently computing partial derivatives of an approximating function $F(w,x)$ realized by the network, adjusting its weights according to the input [Haykin 2009]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Detectors\n",
    "\n",
    "The detection of change is a fundamental theme for a lot of problems related to security, given that the distribution of the data is in constant change to evade security systems. These changes are called concept drift, the situation in which the relation between the input data and the target variable (variable that needs to be learned, which is often the class variable), changes over time. It generally happens when there is a change in a hidden context, which makes it difficult to handle, since this problem spans across different research \n",
    "fields. Each example in the input data is represented by a feature vector $x=[x_1,x_2,...,x_L]$, where $L$ is the number of features that are used to determine it's class $y$, according to the *a posteriori* probabilities $P(y,x)$. $P(x)$ is defined as features distribution and $P(y)$ as prior probabilities. In the literature, there are two types of concept drift, both described below [Wang et al. 2011, Gama et al. 2014]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The virtual concept drift happens when the distribution of the incoming data changes, i.e., $p(x)$ changes, without changing $p(y,x)$. Figure below shows an example of virtual drift with changes in the data distribution at times $t$ and $t+1$, where the red class is more prevalent but it does not lead to changes in the best boundary between them [Almeida et al. 2015]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/drift1.png\" width=\"800\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the problem caused by the virtual concept drift, figure below presents the distribution of a feature for the blue and red classes (presented in previous figure) at both \n",
    "times $t$ and $t+1$. The vertical dashed line in both figures is the threshold that separates both classes (the misclassification cost of the red class is higher than the blue one), that are equally distributed in $t$, different from $t+1$, which has greater presence of the red class. Despite of the fact that the mean and standard deviation of both classes remains unchanged, with the threshold in the same position, i.e., keeping the same classifier unchanged, the probability of finding a red object as a blue one is increased, as shown in the \n",
    "dark red area [Almeida et al. 2015]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/drift2.png\" width=\"800\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real concept drift happens when there is a change in $P(y,x)$ with or without changing $p(x)$, i.e., the relation between the classes and the feature vectors changes over time. A classic example is related to the e-mail spam problem, where an e-mail represented by a feature vector $x_{e}$ can be considered as a spam at a given time $t$ and cannot be at $t+1$, due to user behavior changes. As an example of real concept drift, figure below shows a two class problem with a *a posteriori* probabilities drift, causing changes in the boundaries at time $t$ and $t+1$ and forcing an update in the classifier [Almeida et al. 2015]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/drift3.png\" width=\"800\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both types of concept drift, there are detectors, each one using a different strategy to analyze the changes that occur as time goes by. In this course, we selected three detectors that are commonly used in the literature: DDM (Drift Detection Method [Gama et al. 2004]), EDDM (Early Drift Detection Method [Baena-Garcıa et al. 2006]) and ADWIN (ADaptive WINdowing [Bifet and Gavaldà 2007])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drift Detection Method (DDM)\n",
    "\n",
    "The general idea of Drift Detection Method (DDM) is to monitor the error rate of a model while the samples are presented to the classifier in sequence, as a data stream [Gama et al. 2014]. Usually, when the data distribution is stationary, the error should drop or keep stable as more data is used. When the error rate increases, DDM uses it as evidence to detect a concept change [Albert Bifet 2018]. Given $p_{t}$ the error rate of the classifier and $s_{t}=\\sqrt{\\frac{{}p_{t}(1-p_{t})}{t}}$ the standard deviation, both at time $t$, DDM saves the lowest error rate $p_{min}$ and lowest standard deviation $s_{min}$ observed until time $t$ and then performs the following checks [Albert Bifet 2018]:\n",
    "\n",
    "* If $p_{t}+s{t} \\geq p_{min}+2*s_{min}$, it is considered that the model is in a warning stage, suggesting that a drift is starting. After this point, all the samples are stored in a buffer, as a change may be about to happen.\n",
    "* If $p_{t}+s{t} \\geq p_{min}+3*s_{min}$, it is considered that a change happened. The model used is discarded and a new one is built using the samples stored in the buffer since the warning happened. The values of $p_{min}$ and $s_{min}$ are reseted.\n",
    "\n",
    "Although generic and simple to use, DDM may be too slow to respond to changes in some cases, as many samples can be observed until the drift level is effectively activated, and can even store too many samples in memory in this warning-drift interval [Albert Bifet 2018]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Drift Detection Method (EDDM)\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADaptive WINdowing (ADWIN)\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density-Based Spatial Clustering of Applications with Noise (DBScan)\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# dataset location\n",
    "data_path = \"./datasets/brazilian-malware.csv\"\n",
    "# read CSV dataset\n",
    "data = pd.read_csv(data_path, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical attributes\n",
    "NUMERICAL_ATTRIBUTES = ['BaseOfCode', 'BaseOfData', 'Characteristics', 'DllCharacteristics', \n",
    "                      'Entropy', 'FileAlignment', 'ImageBase', 'Machine', 'Magic',\n",
    "                      'NumberOfRvaAndSizes', 'NumberOfSections', 'NumberOfSymbols', 'PE_TYPE',\n",
    "                      'PointerToSymbolTable', 'Size', 'SizeOfCode', 'SizeOfHeaders',\n",
    "                      'SizeOfImage', 'SizeOfInitializedData', 'SizeOfOptionalHeader',\n",
    "                      'SizeOfUninitializedData']\n",
    "\n",
    "# textual attributes\n",
    "TEXTUAL_ATTRIBUTES = ['Identify', 'ImportedDlls', 'ImportedSymbols']\n",
    "\n",
    "# label used to classify\n",
    "LABEL = 'Label'\n",
    "\n",
    "# attributes that are not used\n",
    "UNUSED_ATTRIBUTES = ['FirstSeenDate', 'SHA1', 'TimeDateStamp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = data[LABEL].values\n",
    "# remove unused attributes and label\n",
    "for a in UNUSED_ATTRIBUTES:\n",
    "    del data[a]\n",
    "del data[LABEL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in half\n",
    "def split_data(data):\n",
    "    # get mid of data\n",
    "    mid = int((len(data) + 1)/2)\n",
    "    # split data into train and test\n",
    "    train_data = data[:mid]\n",
    "    test_data = data[mid:]\n",
    "    # return train and test data\n",
    "    return(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, _ = split_data(data)\n",
    "# label, _ = split_data(label)\n",
    "train_data, test_data = split_data(data)\n",
    "train_label, test_label = split_data(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# extract features from textual attributes\n",
    "def textual_feature_extraction(train_data, test_data, extractor=TfidfVectorizer(max_features=100)):\n",
    "    vectorizer = extractor\n",
    "    # train vectorizer\n",
    "    vectorizer.fit(train_data)\n",
    "    # transform train and test data to features\n",
    "    train_features = vectorizer.transform(train_data)\n",
    "    test_features = vectorizer.transform(test_data)\n",
    "    # return train and test features\n",
    "    return(train_features, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain numerical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_data[NUMERICAL_ATTRIBUTES].values\n",
    "test_features = test_data[NUMERICAL_ATTRIBUTES].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25091, 21), (25090, 21))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape, test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain textual attributes and append to features array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# extract features from each textual attribute\n",
    "for a in TEXTUAL_ATTRIBUTES:\n",
    "    # extract features from current attribute\n",
    "    train_texts, test_texts = textual_feature_extraction(train_data[a], test_data[a])\n",
    "    train_features = np.concatenate((train_features, train_texts.toarray()), axis=1)\n",
    "    test_features = np.concatenate((test_features, test_texts.toarray()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25091, 321), (25090, 321))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape, test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def normalization(train_features, test_features, scaler=MinMaxScaler()):\n",
    "    # train minmax\n",
    "    scaler.fit(train_features)\n",
    "    # transform features\n",
    "    train_features_norm = scaler.transform(train_features)\n",
    "    test_features_norm = scaler.transform(test_features)\n",
    "    # return normalized train and test features\n",
    "    return(train_features_norm, test_features_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_norm, test_features_norm = normalization(train_features, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12545,) (12545,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# initialize classifier\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "# train classifier\n",
    "clf.fit(train_features_norm, train_label)\n",
    "# predict test classes\n",
    "test_pred = clf.predict(test_features_norm)\n",
    "# print test pred and real labels shape\n",
    "print(test_pred.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12545,) (12545,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# initialize classifier\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "# train classifier\n",
    "clf.fit(train_features_norm, train_label)\n",
    "# predict test classes\n",
    "test_pred = clf.predict(test_features_norm)\n",
    "# print test pred and real labels shape\n",
    "print(test_pred.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12545,) (12545,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# initialize classifier\n",
    "clf = SVC(kernel=\"linear\")\n",
    "# train classifier\n",
    "clf.fit(train_features_norm, train_label)\n",
    "# predict test classes\n",
    "test_pred = clf.predict(test_features_norm)\n",
    "# print test pred and real labels shape\n",
    "print(test_pred.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12546,) (12546,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# initialize kmeans\n",
    "clustering = KMeans(n_clusters=2)\n",
    "# fit kmeans (note that we do not need to use labels here)\n",
    "# and predict train classes using the clusters created\n",
    "train_pred = clustering.fit_predict(train_features_norm)\n",
    "# print train pred and real labels shape\n",
    "print(train_pred.shape, train_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBScan\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Multiflow\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNAdwin(leaf_size=30, max_window_size=9223372036854775807, n_neighbors=3,\n",
       "         nominal_attributes=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skmultiflow.lazy.knn_adwin import KNNAdwin\n",
    "from skmultiflow.data import DataStream\n",
    "# initialize classifier\n",
    "clf = KNNAdwin(n_neighbors=3) # disable_weighted_vote=False produces a bug\n",
    "# fit classifier\n",
    "clf.partial_fit(train_features_norm, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a stream with test features\n",
    "stream = DataStream(test_features_norm, test_label)\n",
    "# prepare stream for use\n",
    "stream.prepare_for_use()\n",
    "# create prediction array\n",
    "test_pred = []\n",
    "# iterate over stream\n",
    "while stream.has_more_samples():\n",
    "    # get next sample features and label from stream\n",
    "    sample_features, sample_label = stream.next_sample(1)\n",
    "    # predict sample\n",
    "    sample_pred = clf.predict(sample_features)\n",
    "    # add predicted labels to test_pred\n",
    "    for l in sample_pred:\n",
    "        test_pred.append(l)\n",
    "    # update model with new sample\n",
    "    clf.partial_fit(sample_features, sample_label)\n",
    "# turn test_pred into numpy array\n",
    "test_pred = np.array(test_pred)\n",
    "print(test_pred.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaptiveRandomForest(binary_split=False, disable_weighted_vote=True,\n",
       "                     drift_detection_method=ADWIN(delta=0.001), grace_period=50,\n",
       "                     lambda_value=6, leaf_prediction='nba',\n",
       "                     max_byte_size=33554432, max_features=18,\n",
       "                     memory_estimate_period=2000000, n_estimators=10,\n",
       "                     nb_threshold=0, no_preprune=False, nominal_attributes=None,\n",
       "                     performance_metric='acc', random_state=None,\n",
       "                     remove_poor_atts=False, split_confidence=0.01,\n",
       "                     split_criterion='info_gain', stop_mem_management=False,\n",
       "                     tie_threshold=0.05,\n",
       "                     warning_detection_method=ADWIN(delta=0.01))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skmultiflow.meta import AdaptiveRandomForest\n",
    "from skmultiflow.data import DataStream\n",
    "# initialize classifier\n",
    "clf = AdaptiveRandomForest(n_estimators=10, disable_weighted_vote=True) # disable_weighted_vote=False produces a bug\n",
    "# fit classifier\n",
    "clf.partial_fit(train_features_norm, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12545,) (12545,)\n"
     ]
    }
   ],
   "source": [
    "# create a stream with test features\n",
    "stream = DataStream(test_features_norm, test_label)\n",
    "# prepare stream for use\n",
    "stream.prepare_for_use()\n",
    "# create prediction array\n",
    "test_pred = []\n",
    "# iterate over stream\n",
    "while stream.has_more_samples():\n",
    "    # get next sample features and label from stream\n",
    "    sample_features, sample_label = stream.next_sample(1)\n",
    "    # predict sample\n",
    "    sample_pred = clf.predict(sample_features)\n",
    "    # add predicted labels to test_pred\n",
    "    for l in sample_pred:\n",
    "        test_pred.append(l)\n",
    "    # update model with new sample\n",
    "    clf.partial_fit(sample_features, sample_label)\n",
    "# turn test_pred into numpy array\n",
    "test_pred = np.array(test_pred)\n",
    "print(test_pred.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drift Detectors\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultiflow.trees import HoeffdingTree\n",
    "# initialize classifier\n",
    "clf = HoeffdingTree()\n",
    "# fit classifier\n",
    "clf.partial_fit(train_features_norm, train_label)\n",
    "# initialize classifier 2\n",
    "clf2 = HoeffdingTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultiflow.drift_detection import EDDM, DDM\n",
    "# initialize drift detector\n",
    "drift = DDM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultiflow.data import DataStream\n",
    "from skmultiflow.core import clone\n",
    "# create a stream with test features\n",
    "stream = DataStream(test_features_norm, test_label)\n",
    "# prepare stream for use\n",
    "stream.prepare_for_use()\n",
    "# create prediction array\n",
    "test_pred = []\n",
    "# counter\n",
    "count = 0\n",
    "# drift points\n",
    "drifts = []\n",
    "# iterate over stream\n",
    "while stream.has_more_samples():\n",
    "    # get next sample features and label from stream\n",
    "    sample_features, sample_label = stream.next_sample(1)\n",
    "    # increase counter\n",
    "    count += 1\n",
    "    # predict sample\n",
    "    sample_pred = clf.predict(sample_features)\n",
    "    # add predicted labels to test_pred\n",
    "    for l in sample_pred:\n",
    "        test_pred.append(l)\n",
    "    # add element to drift detector\n",
    "    for e in sample_label == sample_pred:\n",
    "        drift.add_element(e)\n",
    "    # detect if warning or drift\n",
    "    if drift.detected_warning_zone():\n",
    "        # update classifier 2\n",
    "        clf2.partial_fit(sample_features, sample_label)\n",
    "    if drift.detected_change():\n",
    "        # save drift point to array\n",
    "        drifts.append(count)\n",
    "        # change classifiers\n",
    "        clf = clone(clf2)\n",
    "        # initialize classifier 2 again\n",
    "        clf2 = HoeffdingTree()\n",
    "        # reset drift detector\n",
    "        drift.reset()\n",
    "    # update model with new sample\n",
    "    clf.partial_fit(sample_features, sample_label)\n",
    "# turn test_pred into numpy array\n",
    "test_pred = np.array(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected drifts in points [19936, 20938, 21126, 21183, 24425, 24493, 24536, 24729].\n",
      "(25090,) (25090,)\n"
     ]
    }
   ],
   "source": [
    "# print drift points\n",
    "print(\"Detected drifts in points {}.\".format((drifts)))\n",
    "# print shape\n",
    "print(test_pred.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Layer Perceptron\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8405 samples, validate on 4141 samples\n",
      "Epoch 1/10\n",
      "8405/8405 [==============================] - 1s 106us/step - loss: 0.0313 - acc: 0.9861 - val_loss: 5.1686 - val_acc: 0.5576\n",
      "Epoch 2/10\n",
      "8405/8405 [==============================] - 0s 36us/step - loss: 4.0635e-05 - acc: 1.0000 - val_loss: 6.4903 - val_acc: 0.5576\n",
      "Epoch 3/10\n",
      "8405/8405 [==============================] - 0s 37us/step - loss: 1.4403e-06 - acc: 1.0000 - val_loss: 6.9533 - val_acc: 0.5576\n",
      "Epoch 4/10\n",
      "8405/8405 [==============================] - 0s 37us/step - loss: 1.8051e-07 - acc: 1.0000 - val_loss: 7.0748 - val_acc: 0.5576\n",
      "Epoch 5/10\n",
      "8405/8405 [==============================] - 0s 37us/step - loss: 1.2155e-07 - acc: 1.0000 - val_loss: 7.0901 - val_acc: 0.5576\n",
      "Epoch 6/10\n",
      "8405/8405 [==============================] - 0s 35us/step - loss: 1.1462e-07 - acc: 1.0000 - val_loss: 7.0918 - val_acc: 0.5576\n",
      "Epoch 7/10\n",
      "8405/8405 [==============================] - 0s 39us/step - loss: 1.1261e-07 - acc: 1.0000 - val_loss: 7.0919 - val_acc: 0.5576\n",
      "Epoch 8/10\n",
      "8405/8405 [==============================] - 0s 35us/step - loss: 1.1165e-07 - acc: 1.0000 - val_loss: 7.0919 - val_acc: 0.5576\n",
      "Epoch 9/10\n",
      "8405/8405 [==============================] - 0s 33us/step - loss: 1.1113e-07 - acc: 1.0000 - val_loss: 7.0919 - val_acc: 0.5576\n",
      "Epoch 10/10\n",
      "8405/8405 [==============================] - 0s 37us/step - loss: 1.1081e-07 - acc: 1.0000 - val_loss: 7.0919 - val_acc: 0.5576\n",
      "(12545,) (12545,)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# converts labels to a categorical one-hot-vector\n",
    "train_label_onehot = keras.utils.to_categorical(train_label, num_classes=2)\n",
    "test_label_onehot = keras.utils.to_categorical(test_label, num_classes=2)\n",
    "# initialize sequential network\n",
    "model = Sequential()\n",
    "# add fully-connected hidden layer with 200 units\n",
    "model.add(Dense(200, activation='relu', input_dim=train_features_norm.shape[1]))\n",
    "# add fully-connected hidden layer with 100 units\n",
    "model.add(Dense(100, activation='relu'))\n",
    "# output layer\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "# fit model with data\n",
    "model.fit(train_features_norm, train_label_onehot, validation_split=0.33, epochs=10, batch_size=128)\n",
    "# predict classes\n",
    "test_pred = model.predict_classes(test_features_norm)\n",
    "print(test_pred.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Albert Bifet 2018] Albert Bifet, Ricard Gavalda, G. H. B. P. (2018). Machine Learning for Data Streams with Practical Examples in MOA. MIT Press. https://moa. cms.waikato.ac.nz/book/.\n",
    "\n",
    "\n",
    "[Almeida et al. 2015] Almeida, P., Oliveira, L., Britto, A., and Sabourin, R. (2015). Dealing with concept drifts using dynamic ensembles of classifiers. Tesis presented as partial requirement for the degree of Doctor. Graduate Program in Informatics, Sector of Exact Sciences, Universidade Federal do Paraná.\n",
    "\n",
    "[Baena-Garcıa et al. 2006] ´ Baena-Garcıa, M., del Campo-Ávila, J., Fidalgo, R., Bifet, ´A., Gavaldà, R., and Morales-Bueno, R. (2006). Early drift detection method.\n",
    "\n",
    "[Bifet and Gavaldà 2007] Bifet, A. and Gavaldà, R. (2007). Learning from timechanging data with adaptive windowing. volume 7.\n",
    "\n",
    "[Bishop 2006] Bishop, C. M. (2006). Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag, Berlin, Heidelberg.\n",
    "\n",
    "[Breiman 2001] Breiman, L. (2001). Random forests. Mach. Learn., 45(1):5–32.\n",
    "\n",
    "[Gama et al. 2004] Gama, J., Medas, P., Castillo, G., and Rodrigues, P. (2004). Learning with drift detection. In Bazzan, A. L. C. and Labidi, S., editors, Advances in Artificial Intelligence – SBIA 2004, pages 286–295, Berlin, Heidelberg. Springer Berlin Heidelberg.\n",
    "\n",
    "[Gama et al. 2014] Gama, J. a., Žliobaite, I., Bifet, A., Pechenizkiy, M., and Bouchachia, ˙A. (2014). A survey on concept drift adaptation. ACM Comput. Surv., 46(4):44:1–44:37.\n",
    "\n",
    "[Haykin 2009] Haykin, S. (2009). Neural Networks and Learning Machines. Number v. 10 in Neural networks and learning machines. Prentice Hall.\n",
    "\n",
    "[Manning et al. 2008a] Manning, C. D., Raghavan, P., and Schütze, H. (2008a). Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA.\n",
    "\n",
    "[Michie et al. 1994] Michie, D., Spiegelhalter, D. J., Taylor, C. C., and Campbell, J., editors (1994). Machine Learning, Neural and Statistical Classification. Ellis Horwood, Upper Saddle River, NJ, USA.\n",
    "\n",
    "[Milgram et al. 2006] Milgram, J., Cheriet, M., and Sabourin, R. (2006). “One Against One” or “One Against All”: Which One is Better for Handwriting Recognition with SVMs? In Lorette, G., editor, Tenth International Workshop on Frontiers in Handwriting Recognition, La Baule (France). Université de Rennes 1, Suvisoft. http://www.suvisoft.com.\n",
    "\n",
    "[Mitchell 1997] Mitchell, T. M. (1997). Machine Learning. McGraw-Hill, Inc., New York, NY, USA, 1 edition.\n",
    "\n",
    "[Wang et al. 2011] Wang, S., Schlobach, S., and Klein, M. (2011). Concept drift and how to identify it. Web Semantics: Science, Services and Agents on the World Wide Web, 9(3):247 – 265. Semantic Web Dynamics Semantic Web Challenge, 2010."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**<< Previous Section**](04_features.ipynb) | [**Next Section >>**](06_evaluation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
