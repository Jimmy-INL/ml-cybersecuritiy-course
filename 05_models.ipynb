{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Models\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detectors\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# dataset location\n",
    "data_path = \"./datasets/brazilian-malware.csv\"\n",
    "# read CSV dataset\n",
    "data = pd.read_csv(data_path, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical attributes\n",
    "NUMERICAL_ATTRIBUTES = ['BaseOfCode', 'BaseOfData', 'Characteristics', 'DllCharacteristics', \n",
    "                      'Entropy', 'FileAlignment', 'ImageBase', 'Machine', 'Magic',\n",
    "                      'NumberOfRvaAndSizes', 'NumberOfSections', 'NumberOfSymbols', 'PE_TYPE',\n",
    "                      'PointerToSymbolTable', 'Size', 'SizeOfCode', 'SizeOfHeaders',\n",
    "                      'SizeOfImage', 'SizeOfInitializedData', 'SizeOfOptionalHeader',\n",
    "                      'SizeOfUninitializedData']\n",
    "\n",
    "# textual attributes\n",
    "TEXTUAL_ATTRIBUTES = ['Identify', 'ImportedDlls', 'ImportedSymbols']\n",
    "\n",
    "# label used to classify\n",
    "LABEL = 'Label'\n",
    "\n",
    "# attributes that are not used\n",
    "UNUSED_ATTRIBUTES = ['FirstSeenDate', 'SHA1', 'TimeDateStamp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = data[LABEL].values\n",
    "# remove unused attributes and label\n",
    "for a in UNUSED_ATTRIBUTES:\n",
    "    del data[a]\n",
    "del data[LABEL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in half\n",
    "def split_data(data):\n",
    "    # get mid of data\n",
    "    mid = int((len(data) + 1)/2)\n",
    "    # split data into train and test\n",
    "    train_data = data[:mid]\n",
    "    test_data = data[mid:]\n",
    "    # return train and test data\n",
    "    return(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, _ = split_data(data)\n",
    "label, _ = split_data(label)\n",
    "train_data, test_data = split_data(data)\n",
    "train_label, test_label = split_data(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# extract features from textual attributes\n",
    "def textual_feature_extraction(train_data, test_data, extractor=TfidfVectorizer(max_features=100)):\n",
    "    vectorizer = extractor\n",
    "    # train vectorizer\n",
    "    vectorizer.fit(train_data)\n",
    "    # transform train and test data to features\n",
    "    train_features = vectorizer.transform(train_data)\n",
    "    test_features = vectorizer.transform(test_data)\n",
    "    # return train and test features\n",
    "    return(train_features, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain numerical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_data[NUMERICAL_ATTRIBUTES].values\n",
    "test_features = test_data[NUMERICAL_ATTRIBUTES].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12546, 21), (12545, 21))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape, test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain textual attributes and append to features array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# extract features from each textual attribute\n",
    "for a in TEXTUAL_ATTRIBUTES:\n",
    "    # extract features from current attribute\n",
    "    train_texts, test_texts = textual_feature_extraction(train_data[a], test_data[a])\n",
    "    train_features = np.concatenate((train_features, train_texts.toarray()), axis=1)\n",
    "    test_features = np.concatenate((test_features, test_texts.toarray()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12546, 321), (12545, 321))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape, test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def normalization(train_features, test_features, scaler=MinMaxScaler()):\n",
    "    # train minmax\n",
    "    scaler.fit(train_features)\n",
    "    # transform features\n",
    "    train_features_norm = scaler.transform(train_features)\n",
    "    test_features_norm = scaler.transform(test_features)\n",
    "    # return normalized train and test features\n",
    "    return(train_features_norm, test_features_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_norm, test_features_norm = normalization(train_features, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12545,) (12545,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# initialize classifier\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "# train classifier\n",
    "clf.fit(train_features_norm, train_label)\n",
    "# predict test classes\n",
    "test_pred = clf.predict(test_features_norm)\n",
    "# print test pred and real labels shape\n",
    "print(test_pred.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12545,) (12545,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# initialize classifier\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "# train classifier\n",
    "clf.fit(train_features_norm, train_label)\n",
    "# predict test classes\n",
    "test_pred = clf.predict(test_features_norm)\n",
    "# print test pred and real labels shape\n",
    "print(test_pred.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12545,) (12545,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# initialize classifier\n",
    "clf = SVC(kernel=\"linear\")\n",
    "# train classifier\n",
    "clf.fit(train_features_norm, train_label)\n",
    "# predict test classes\n",
    "test_pred = clf.predict(test_features_norm)\n",
    "# print test pred and real labels shape\n",
    "print(test_pred.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12546,) (12546,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# initialize kmeans\n",
    "clustering = KMeans(n_clusters=2)\n",
    "# fit kmeans (note that we do not need to use labels here)\n",
    "# and predict train classes using the clusters created\n",
    "train_pred = clustering.fit_predict(train_features_norm)\n",
    "# print train pred and real labels shape\n",
    "print(train_pred.shape, train_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBScan\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Multiflow\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaptiveRandomForest(binary_split=False, disable_weighted_vote=True,\n",
       "                     drift_detection_method=ADWIN(delta=0.001), grace_period=50,\n",
       "                     lambda_value=6, leaf_prediction='nba',\n",
       "                     max_byte_size=33554432, max_features=18,\n",
       "                     memory_estimate_period=2000000, n_estimators=10,\n",
       "                     nb_threshold=0, no_preprune=False, nominal_attributes=None,\n",
       "                     performance_metric='acc', random_state=None,\n",
       "                     remove_poor_atts=False, split_confidence=0.01,\n",
       "                     split_criterion='info_gain', stop_mem_management=False,\n",
       "                     tie_threshold=0.05,\n",
       "                     warning_detection_method=ADWIN(delta=0.01))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skmultiflow.meta import AdaptiveRandomForest\n",
    "from skmultiflow.data import DataStream\n",
    "# initialize classifier\n",
    "clf = AdaptiveRandomForest(n_estimators=10, disable_weighted_vote=True) # disable_weighted_vote=False produces a bug\n",
    "# fit classifier\n",
    "clf.partial_fit(train_features_norm, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12545,) (12545,)\n"
     ]
    }
   ],
   "source": [
    "# create a stream with test features\n",
    "stream = DataStream(test_features_norm, test_label)\n",
    "# prepare stream for use\n",
    "stream.prepare_for_use()\n",
    "# create prediction array\n",
    "test_pred = []\n",
    "# iterate over stream\n",
    "while stream.has_more_samples():\n",
    "    # get next sample features and label from stream\n",
    "    sample_features, sample_label = stream.next_sample(1)\n",
    "    # predict sample\n",
    "    sample_pred = clf.predict(sample_features)\n",
    "    # add predicted labels to test_pred\n",
    "    for l in sample_pred:\n",
    "        test_pred.append(l)\n",
    "    # update model with new sample\n",
    "    clf.partial_fit(sample_features, sample_label)\n",
    "# turn test_pred into numpy array\n",
    "test_pred = np.array(test_pred)\n",
    "print(test_pred.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Layer Perceptron\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8405 samples, validate on 4141 samples\n",
      "Epoch 1/10\n",
      "8405/8405 [==============================] - 1s 95us/step - loss: 0.0219 - acc: 0.9996 - val_loss: 5.3590 - val_acc: 0.5576\n",
      "Epoch 2/10\n",
      "8405/8405 [==============================] - 0s 40us/step - loss: 3.4011e-05 - acc: 1.0000 - val_loss: 6.6105 - val_acc: 0.5576\n",
      "Epoch 3/10\n",
      "8405/8405 [==============================] - 0s 35us/step - loss: 1.0791e-06 - acc: 1.0000 - val_loss: 6.9370 - val_acc: 0.5576\n",
      "Epoch 4/10\n",
      "8405/8405 [==============================] - 0s 34us/step - loss: 1.6115e-07 - acc: 1.0000 - val_loss: 7.0641 - val_acc: 0.5576\n",
      "Epoch 5/10\n",
      "8405/8405 [==============================] - 0s 35us/step - loss: 1.1825e-07 - acc: 1.0000 - val_loss: 7.0764 - val_acc: 0.5576\n",
      "Epoch 6/10\n",
      "8405/8405 [==============================] - 0s 37us/step - loss: 1.1381e-07 - acc: 1.0000 - val_loss: 7.0839 - val_acc: 0.5576\n",
      "Epoch 7/10\n",
      "8405/8405 [==============================] - 0s 33us/step - loss: 1.1218e-07 - acc: 1.0000 - val_loss: 7.0877 - val_acc: 0.5576\n",
      "Epoch 8/10\n",
      "8405/8405 [==============================] - 0s 33us/step - loss: 1.1140e-07 - acc: 1.0000 - val_loss: 7.0906 - val_acc: 0.5576\n",
      "Epoch 9/10\n",
      "8405/8405 [==============================] - 0s 33us/step - loss: 1.1098e-07 - acc: 1.0000 - val_loss: 7.0916 - val_acc: 0.5576\n",
      "Epoch 10/10\n",
      "8405/8405 [==============================] - 0s 37us/step - loss: 1.1068e-07 - acc: 1.0000 - val_loss: 7.0918 - val_acc: 0.5576\n",
      "(12545,) (12545,)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# converts labels to a categorical one-hot-vector\n",
    "train_label_onehot = keras.utils.to_categorical(train_label, num_classes=2)\n",
    "test_label_onehot = keras.utils.to_categorical(test_label, num_classes=2)\n",
    "# initialize sequential network\n",
    "model = Sequential()\n",
    "# add fully-connected hidden layer with 200 units\n",
    "model.add(Dense(200, activation='relu', input_dim=train_features_norm.shape[1]))\n",
    "# add fully-connected hidden layer with 100 units\n",
    "model.add(Dense(100, activation='relu'))\n",
    "# output layer\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "# fit model with data\n",
    "model.fit(train_features_norm, train_label_onehot, validation_split=0.33, epochs=10, batch_size=128)\n",
    "# predict classes\n",
    "test_pred = model.predict_classes(test_features_norm)\n",
    "print(test_pred.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
