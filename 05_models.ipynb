{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Models\n",
    "\n",
    "In this section we present the most used machine learning models in problems involving security, including their theory, algorithms and codes of how to use them. Among the models, there are classifiers, detectors and clustering techniques, all of them explained here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Classifiers\n",
    "\n",
    "Classifiers aim to classify a given input sample into a previously known class by them during the training. The training is the step that the classifier learns the patterns of each class with the data presented to it (together with their labels), adapting its parameters to the problem. This type of problem is known as supervised learning [Bishop 2006]. After the training, the model can be used to classify any unknown data, allowing it to be effectively used. Here we present the following classifiers: K-Nearest Neighbors, based in neighborhood, Random Forest, an ensemble based in decision trees, Support Vector Machines, based in the construction of a optimal hyperplane, and Multi-Layer Perceptron, a neural network type used a lot in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN)\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a machine learning model based in distance, which classification of a new instance is based on the distance of the $k$ training samples most closer to a given testing sample. Thus, a new unknown sample will be classified as being from the most prevalent classes between these $k$ samples, as shown in figure below [Michie et al. 1994], where a new instance will be classified as red when $k=3$, green, when $k=5$ and unknown when $k=6$ (the result is a draw, that is why even numbers are not recommended in binary classification - when there are just two classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/knn1.png\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, the distance used by KNN is the Euclidean distance. Given an instance $x$, described by $(a_{1}(x),a_{2}(x),...,a_{n}(x))$, where $a_{i}(x)$ is the $i$-th feature, the distance between two instances $x_{i}$ and $x_{j}$ is defined by the equation $d(x_{i},x_{j}) = \\sqrt{\\sum\\limits_{r=1}^{n}(a_{r}(x_{i})-a_{r}(x_{j}))^{2})}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron (MLP)\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Detectors\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drift Detection Method (DDM)\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Drift Detection Method (EDDM)\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADaptive WINdowing (ADWIN)\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density-Based Spatial Clustering of Applications with Noise (DBScan)\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# dataset location\n",
    "data_path = \"./datasets/brazilian-malware.csv\"\n",
    "# read CSV dataset\n",
    "data = pd.read_csv(data_path, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical attributes\n",
    "NUMERICAL_ATTRIBUTES = ['BaseOfCode', 'BaseOfData', 'Characteristics', 'DllCharacteristics', \n",
    "                      'Entropy', 'FileAlignment', 'ImageBase', 'Machine', 'Magic',\n",
    "                      'NumberOfRvaAndSizes', 'NumberOfSections', 'NumberOfSymbols', 'PE_TYPE',\n",
    "                      'PointerToSymbolTable', 'Size', 'SizeOfCode', 'SizeOfHeaders',\n",
    "                      'SizeOfImage', 'SizeOfInitializedData', 'SizeOfOptionalHeader',\n",
    "                      'SizeOfUninitializedData']\n",
    "\n",
    "# textual attributes\n",
    "TEXTUAL_ATTRIBUTES = ['Identify', 'ImportedDlls', 'ImportedSymbols']\n",
    "\n",
    "# label used to classify\n",
    "LABEL = 'Label'\n",
    "\n",
    "# attributes that are not used\n",
    "UNUSED_ATTRIBUTES = ['FirstSeenDate', 'SHA1', 'TimeDateStamp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = data[LABEL].values\n",
    "# remove unused attributes and label\n",
    "for a in UNUSED_ATTRIBUTES:\n",
    "    del data[a]\n",
    "del data[LABEL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in half\n",
    "def split_data(data):\n",
    "    # get mid of data\n",
    "    mid = int((len(data) + 1)/2)\n",
    "    # split data into train and test\n",
    "    train_data = data[:mid]\n",
    "    test_data = data[mid:]\n",
    "    # return train and test data\n",
    "    return(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, _ = split_data(data)\n",
    "# label, _ = split_data(label)\n",
    "train_data, test_data = split_data(data)\n",
    "train_label, test_label = split_data(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# extract features from textual attributes\n",
    "def textual_feature_extraction(train_data, test_data, extractor=TfidfVectorizer(max_features=100)):\n",
    "    vectorizer = extractor\n",
    "    # train vectorizer\n",
    "    vectorizer.fit(train_data)\n",
    "    # transform train and test data to features\n",
    "    train_features = vectorizer.transform(train_data)\n",
    "    test_features = vectorizer.transform(test_data)\n",
    "    # return train and test features\n",
    "    return(train_features, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain numerical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_data[NUMERICAL_ATTRIBUTES].values\n",
    "test_features = test_data[NUMERICAL_ATTRIBUTES].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25091, 21), (25090, 21))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape, test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain textual attributes and append to features array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# extract features from each textual attribute\n",
    "for a in TEXTUAL_ATTRIBUTES:\n",
    "    # extract features from current attribute\n",
    "    train_texts, test_texts = textual_feature_extraction(train_data[a], test_data[a])\n",
    "    train_features = np.concatenate((train_features, train_texts.toarray()), axis=1)\n",
    "    test_features = np.concatenate((test_features, test_texts.toarray()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25091, 321), (25090, 321))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape, test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def normalization(train_features, test_features, scaler=MinMaxScaler()):\n",
    "    # train minmax\n",
    "    scaler.fit(train_features)\n",
    "    # transform features\n",
    "    train_features_norm = scaler.transform(train_features)\n",
    "    test_features_norm = scaler.transform(test_features)\n",
    "    # return normalized train and test features\n",
    "    return(train_features_norm, test_features_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_norm, test_features_norm = normalization(train_features, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12545,) (12545,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# initialize classifier\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "# train classifier\n",
    "clf.fit(train_features_norm, train_label)\n",
    "# predict test classes\n",
    "test_pred = clf.predict(test_features_norm)\n",
    "# print test pred and real labels shape\n",
    "print(test_pred.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12545,) (12545,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# initialize classifier\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "# train classifier\n",
    "clf.fit(train_features_norm, train_label)\n",
    "# predict test classes\n",
    "test_pred = clf.predict(test_features_norm)\n",
    "# print test pred and real labels shape\n",
    "print(test_pred.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12545,) (12545,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# initialize classifier\n",
    "clf = SVC(kernel=\"linear\")\n",
    "# train classifier\n",
    "clf.fit(train_features_norm, train_label)\n",
    "# predict test classes\n",
    "test_pred = clf.predict(test_features_norm)\n",
    "# print test pred and real labels shape\n",
    "print(test_pred.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12546,) (12546,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# initialize kmeans\n",
    "clustering = KMeans(n_clusters=2)\n",
    "# fit kmeans (note that we do not need to use labels here)\n",
    "# and predict train classes using the clusters created\n",
    "train_pred = clustering.fit_predict(train_features_norm)\n",
    "# print train pred and real labels shape\n",
    "print(train_pred.shape, train_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBScan\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Multiflow\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNAdwin(leaf_size=30, max_window_size=9223372036854775807, n_neighbors=3,\n",
       "         nominal_attributes=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skmultiflow.lazy.knn_adwin import KNNAdwin\n",
    "from skmultiflow.data import DataStream\n",
    "# initialize classifier\n",
    "clf = KNNAdwin(n_neighbors=3) # disable_weighted_vote=False produces a bug\n",
    "# fit classifier\n",
    "clf.partial_fit(train_features_norm, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a stream with test features\n",
    "stream = DataStream(test_features_norm, test_label)\n",
    "# prepare stream for use\n",
    "stream.prepare_for_use()\n",
    "# create prediction array\n",
    "test_pred = []\n",
    "# iterate over stream\n",
    "while stream.has_more_samples():\n",
    "    # get next sample features and label from stream\n",
    "    sample_features, sample_label = stream.next_sample(1)\n",
    "    # predict sample\n",
    "    sample_pred = clf.predict(sample_features)\n",
    "    # add predicted labels to test_pred\n",
    "    for l in sample_pred:\n",
    "        test_pred.append(l)\n",
    "    # update model with new sample\n",
    "    clf.partial_fit(sample_features, sample_label)\n",
    "# turn test_pred into numpy array\n",
    "test_pred = np.array(test_pred)\n",
    "print(test_pred.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaptiveRandomForest(binary_split=False, disable_weighted_vote=True,\n",
       "                     drift_detection_method=ADWIN(delta=0.001), grace_period=50,\n",
       "                     lambda_value=6, leaf_prediction='nba',\n",
       "                     max_byte_size=33554432, max_features=18,\n",
       "                     memory_estimate_period=2000000, n_estimators=10,\n",
       "                     nb_threshold=0, no_preprune=False, nominal_attributes=None,\n",
       "                     performance_metric='acc', random_state=None,\n",
       "                     remove_poor_atts=False, split_confidence=0.01,\n",
       "                     split_criterion='info_gain', stop_mem_management=False,\n",
       "                     tie_threshold=0.05,\n",
       "                     warning_detection_method=ADWIN(delta=0.01))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skmultiflow.meta import AdaptiveRandomForest\n",
    "from skmultiflow.data import DataStream\n",
    "# initialize classifier\n",
    "clf = AdaptiveRandomForest(n_estimators=10, disable_weighted_vote=True) # disable_weighted_vote=False produces a bug\n",
    "# fit classifier\n",
    "clf.partial_fit(train_features_norm, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12545,) (12545,)\n"
     ]
    }
   ],
   "source": [
    "# create a stream with test features\n",
    "stream = DataStream(test_features_norm, test_label)\n",
    "# prepare stream for use\n",
    "stream.prepare_for_use()\n",
    "# create prediction array\n",
    "test_pred = []\n",
    "# iterate over stream\n",
    "while stream.has_more_samples():\n",
    "    # get next sample features and label from stream\n",
    "    sample_features, sample_label = stream.next_sample(1)\n",
    "    # predict sample\n",
    "    sample_pred = clf.predict(sample_features)\n",
    "    # add predicted labels to test_pred\n",
    "    for l in sample_pred:\n",
    "        test_pred.append(l)\n",
    "    # update model with new sample\n",
    "    clf.partial_fit(sample_features, sample_label)\n",
    "# turn test_pred into numpy array\n",
    "test_pred = np.array(test_pred)\n",
    "print(test_pred.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drift Detectors\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultiflow.trees import HoeffdingTree\n",
    "# initialize classifier\n",
    "clf = HoeffdingTree()\n",
    "# fit classifier\n",
    "clf.partial_fit(train_features_norm, train_label)\n",
    "# initialize classifier 2\n",
    "clf2 = HoeffdingTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultiflow.drift_detection import EDDM, DDM\n",
    "# initialize drift detector\n",
    "drift = DDM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultiflow.data import DataStream\n",
    "from skmultiflow.core import clone\n",
    "# create a stream with test features\n",
    "stream = DataStream(test_features_norm, test_label)\n",
    "# prepare stream for use\n",
    "stream.prepare_for_use()\n",
    "# create prediction array\n",
    "test_pred = []\n",
    "# counter\n",
    "count = 0\n",
    "# drift points\n",
    "drifts = []\n",
    "# iterate over stream\n",
    "while stream.has_more_samples():\n",
    "    # get next sample features and label from stream\n",
    "    sample_features, sample_label = stream.next_sample(1)\n",
    "    # increase counter\n",
    "    count += 1\n",
    "    # predict sample\n",
    "    sample_pred = clf.predict(sample_features)\n",
    "    # add predicted labels to test_pred\n",
    "    for l in sample_pred:\n",
    "        test_pred.append(l)\n",
    "    # add element to drift detector\n",
    "    for e in sample_label == sample_pred:\n",
    "        drift.add_element(e)\n",
    "    # detect if warning or drift\n",
    "    if drift.detected_warning_zone():\n",
    "        # update classifier 2\n",
    "        clf2.partial_fit(sample_features, sample_label)\n",
    "    if drift.detected_change():\n",
    "        # save drift point to array\n",
    "        drifts.append(count)\n",
    "        # change classifiers\n",
    "        clf = clone(clf2)\n",
    "        # initialize classifier 2 again\n",
    "        clf2 = HoeffdingTree()\n",
    "        # reset drift detector\n",
    "        drift.reset()\n",
    "    # update model with new sample\n",
    "    clf.partial_fit(sample_features, sample_label)\n",
    "# turn test_pred into numpy array\n",
    "test_pred = np.array(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected drifts in points [19936, 20938, 21126, 21183, 24425, 24493, 24536, 24729].\n",
      "(25090,) (25090,)\n"
     ]
    }
   ],
   "source": [
    "# print drift points\n",
    "print(\"Detected drifts in points {}.\".format((drifts)))\n",
    "# print shape\n",
    "print(test_pred.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Layer Perceptron\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8405 samples, validate on 4141 samples\n",
      "Epoch 1/10\n",
      "8405/8405 [==============================] - 1s 106us/step - loss: 0.0313 - acc: 0.9861 - val_loss: 5.1686 - val_acc: 0.5576\n",
      "Epoch 2/10\n",
      "8405/8405 [==============================] - 0s 36us/step - loss: 4.0635e-05 - acc: 1.0000 - val_loss: 6.4903 - val_acc: 0.5576\n",
      "Epoch 3/10\n",
      "8405/8405 [==============================] - 0s 37us/step - loss: 1.4403e-06 - acc: 1.0000 - val_loss: 6.9533 - val_acc: 0.5576\n",
      "Epoch 4/10\n",
      "8405/8405 [==============================] - 0s 37us/step - loss: 1.8051e-07 - acc: 1.0000 - val_loss: 7.0748 - val_acc: 0.5576\n",
      "Epoch 5/10\n",
      "8405/8405 [==============================] - 0s 37us/step - loss: 1.2155e-07 - acc: 1.0000 - val_loss: 7.0901 - val_acc: 0.5576\n",
      "Epoch 6/10\n",
      "8405/8405 [==============================] - 0s 35us/step - loss: 1.1462e-07 - acc: 1.0000 - val_loss: 7.0918 - val_acc: 0.5576\n",
      "Epoch 7/10\n",
      "8405/8405 [==============================] - 0s 39us/step - loss: 1.1261e-07 - acc: 1.0000 - val_loss: 7.0919 - val_acc: 0.5576\n",
      "Epoch 8/10\n",
      "8405/8405 [==============================] - 0s 35us/step - loss: 1.1165e-07 - acc: 1.0000 - val_loss: 7.0919 - val_acc: 0.5576\n",
      "Epoch 9/10\n",
      "8405/8405 [==============================] - 0s 33us/step - loss: 1.1113e-07 - acc: 1.0000 - val_loss: 7.0919 - val_acc: 0.5576\n",
      "Epoch 10/10\n",
      "8405/8405 [==============================] - 0s 37us/step - loss: 1.1081e-07 - acc: 1.0000 - val_loss: 7.0919 - val_acc: 0.5576\n",
      "(12545,) (12545,)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# converts labels to a categorical one-hot-vector\n",
    "train_label_onehot = keras.utils.to_categorical(train_label, num_classes=2)\n",
    "test_label_onehot = keras.utils.to_categorical(test_label, num_classes=2)\n",
    "# initialize sequential network\n",
    "model = Sequential()\n",
    "# add fully-connected hidden layer with 200 units\n",
    "model.add(Dense(200, activation='relu', input_dim=train_features_norm.shape[1]))\n",
    "# add fully-connected hidden layer with 100 units\n",
    "model.add(Dense(100, activation='relu'))\n",
    "# output layer\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "# fit model with data\n",
    "model.fit(train_features_norm, train_label_onehot, validation_split=0.33, epochs=10, batch_size=128)\n",
    "# predict classes\n",
    "test_pred = model.predict_classes(test_features_norm)\n",
    "print(test_pred.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Bishop 2006] Bishop, C. M. (2006). Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag, Berlin, Heidelberg.\n",
    "\n",
    "[Michie et al. 1994] Michie, D., Spiegelhalter, D. J., Taylor, C. C., and Campbell, J., editors (1994). Machine Learning, Neural and Statistical Classification. Ellis Horwood, Upper Saddle River, NJ, USA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**<< Previous Section**](04_features.ipynb) | [**Next Section**](06_evaluation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
