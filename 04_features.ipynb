{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Feature Extraction\n",
    "\n",
    "Sometimes the attributes extracted cannot be used directly in a machine learning model. From the attributes presented in previous section, for example, only the numeric attributes (which values are real or integer) could be used directly in a model. The list of words extracted, such as libraries and system calls used by a software, must pass through one more step called feature extraction, whose objective is to transform these attributes into something readable by the classifier, usually defining the same number of features as output of this process (once a program can use more libraries than others, for example). In this section we introduce how to extract features from textual attributes (using TF-IDF, Word2Vec and Encoders) and how to normalize them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "We are going to present two textual feature extraction methods, both of them used a lot in the literature: TF-IDF and Word2Vec. Given a text representing an attribute, both methods produce as output a numeric list representing in a different way this text. It is important to note that the list of words extracted in previous section are transformed into texts, where each word is separated by a space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-DF\n",
    "\n",
    "Given a vocabulary from a set of texts, i.e., all the words that appear in these texts, each text ![i](imgs/tfidf1.png) is represented by a vector ![vec](imgs/tfidf2.png), where ![wij](imgs/tfidf3.png) represents the TF-IDF (Term Frequency - Inverse Document Frequency) from the j-th word of the vocabulary. The TF-IDF is as statistic measure used to evaluate how important a word is to a text in relation to a collection of texts [Manning et al. 2008a]. This measure is obtained through the multiplication of two terms:\n",
    "\n",
    "* **Term Frequency (TF):** measure how often a word ![t](imgs/tfidf4.png) occurs in a text through the equation ![tf](imgs/tfidf5.png).\n",
    "\n",
    "* **Inverse Document Frequency (IDF):** measure how important a word ![t](imgs/tfidf4.png) is through the equation ![idf](imgs/tfidf6.png).\n",
    "\n",
    "Briefly, each text is represented by a sparse array that contains their TF-IDF values for each word in the vocabulary. The vocabulary can be reduced to a number V of words, the most prevalent in the set of texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Brazilian Malware dataset, below we present a way to extract features from the list of libraries used by each sample using TF-IDF, through the class [*TfidfVectorizer*](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) from scikit-learn [Pedregosa et al. 2011]. First, the dataset is read, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# dataset location\n",
    "data_path = \"./datasets/brazilian-malware.csv\"\n",
    "# read CSV dataset\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the extractor is initialized with a maximum of 200 features (each feature represents a word) and the set of samples is divided into two: the first set is used to train the extractor (using the method *fit*, which generates the vocabulary used) and the later, to test. Finally, both sets have their features extracted through the method *transform*. Looking at the output, we can see that the resulting matrix has 200 columns, the maximum number of features we selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25091, 200) (25090, 200)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# get texts\n",
    "texts = data[\"ImportedDlls\"].values\n",
    "# get mid of data\n",
    "mid = int((len(texts) + 1)/2)\n",
    "# split data into train and test\n",
    "train_texts = texts[:mid]\n",
    "test_texts = texts[mid:]\n",
    "# initialize vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=200)\n",
    "# train vectorizer\n",
    "vectorizer.fit(train_texts)\n",
    "# transform train texts to features\n",
    "train_features = vectorizer.transform(train_texts)\n",
    "test_features = vectorizer.transform(test_texts)\n",
    "# print features shape\n",
    "print(train_features.shape, test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "Developed in 2013 by Google researchers [Mikolov et al. 2013a], word2vec is a way to create vectors that represent words. The greatest contribution of this method is its capability of representing the similarity between words and their meanings, different from TF-IDF, which just project data based on their frequency. Besides that, word2vec projects only words and not the text, making necessary to use a strategy to extract features from the projected words. The word2vec general model uses a feed-forward non-supervised neural network (for more details about neural networks, check [section 5](05_models.ipynb)). There are two architectures available for this model: continuous bag-of-words (CBOW) e skip-gram. The first one (CBOW) tries to predict the central word from a set of words around them (input), similar to a cluster. In contrast, the skip-gram architecture does the opposite: it tries to predict the words around, given a central word as input. The figure below presents both methods [Shulman 2016, Mikolov et al. 2013b]. CBOW was the architecture used in this work and will be explained in details further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"imgs/word2vec1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, each input of the network is codified into an vector ![x_ck](imgs/word2vec2.png), where V is the vocabulary size, *c* is the word identifier and *k*, the text identifier (as presented in the figure below). The item ![x_i](imgs/word2vec3.png) is 1 only if *i* corresponds to the word identifier *c*. Otherwise, ![x_i](imgs/word2vec3.png) is 0, i.e., only one position of the array of each word will have value 1 (only the one that corresponds to the word). This codification is called one-hot encoding. Through this input, the model calculates the average of these arrays and multiplies it by the weights from the hidden layer *h*, generating the output *y*. More details about the training of this network can be found in [Rong 2014]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"imgs/word2vec4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code bellow presents a way to extract features using word2vec. For that, we implemented a class called *MeanEmbeddingVectorizer*, which uses the model [*Word2Vec*](https://radimrehurek.com/gensim/models/word2vec.html) implemented by the library gensim [Rehurek and Sojka 2010]. In this class, we defined only two parameters (more parameters can be added when modifying this class: *size*, which defines the dimension used to project the words and *min_count*, which defines the minimum of apparition for a word to be considered. In the method *fit* we train the *Word2Vec* model with the train data and obtain the projection of these data. With the method *transform*, the data passed as parameter use the created projection to create a feature vector for each sample, based on the average of their words (in case the word does not exist, it is considered as being a null vector). Thus, the class that we implemented works very similar to the one implemented by TF-IDF's scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25091, 200) (25090, 200)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# vectorizer class: calc average of words using word2vec\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, size, min_count=1): #word2vec):\n",
    "        self.size = size\n",
    "        self.min_count = 1\n",
    "\n",
    "    def fit(self, X):\n",
    "        w2v = Word2Vec(X,size=self.size,min_count=self.min_count)\n",
    "        self.word2vec = dict(zip(w2v.wv.index2word,w2v.wv.vectors))\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(list(self.word2vec.values())[0])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "# get texts\n",
    "texts = data[\"ImportedDlls\"].values\n",
    "# get mid of data\n",
    "mid = int((len(texts) + 1)/2)\n",
    "# split data into train and test\n",
    "train_texts = texts[:mid]\n",
    "test_texts = texts[mid:]\n",
    "# initialize vectorizer\n",
    "m = MeanEmbeddingVectorizer(size=200)\n",
    "# train vectorizer\n",
    "m.fit(train_texts)\n",
    "# transform train and test texts to w2v mean\n",
    "train_features = m.transform(train_texts)\n",
    "test_features = m.transform(test_texts)\n",
    "# print features shape\n",
    "print(train_features.shape, test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-Max\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25091, 200) (25090, 200)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# initialize minmax\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# train minmax\n",
    "scaler.fit(train_features.toarray())\n",
    "# transform features\n",
    "train_features_norm = scaler.transform(train_features.toarray())\n",
    "test_features_norm = scaler.transform(test_features.toarray())\n",
    "print(train_features_norm.shape, test_features_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# initialize minmax\n",
    "scaler = StandardScaler()\n",
    "# train minmax\n",
    "scaler.fit(train_features.toarray())\n",
    "# transform features\n",
    "train_features_norm = scaler.transform(train_features.toarray())\n",
    "test_features_norm = scaler.transform(test_features.toarray())\n",
    "print(train_features_norm.shape, test_features_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Manning et al. 2008a] Manning, C. D., Raghavan, P., and Schütze, H. (2008a). Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA.\n",
    "\n",
    "[Mikolov et al. 2013a] Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013a). Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.\n",
    "\n",
    "[Pedregosa et al. 2011] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. (2011). Scikitlearn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.\n",
    "\n",
    "[Rehurek and Sojka 2010] Reh ˚u ˇ ˇrek, R. and Sojka, P. (2010). Software Framework for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45–50, Valletta, Malta. ELRA. http: //is.muni.cz/publication/884893/en.\n",
    "\n",
    "[Rong 2014] Rong, X. (2014). word2vec parameter learning explained. CoRR, abs/1411.2738."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
