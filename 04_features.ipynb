{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Feature Extraction\n",
    "\n",
    "Sometimes the attributes extracted cannot be used directly in a machine learning model. From the attributes presented in previous section, for example, only the numeric attributes (which values are real or integer) could be used directly in a model. The list of words extracted, such as libraries and system calls used by a software, must pass through one more step called feature extraction, whose objective is to transform these attributes into something readable by the classifier, usually defining the same number of features as output of this process (once a program can use more libraries than others, for example). In this section we introduce how to extract features from textual attributes (using TF-IDF, Word2Vec and Encoders) and how to normalize them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "We are going to present two textual feature extraction methods, both of them used a lot in the literature: TF-IDF and Word2Vec. Given a text representing an attribute, both methods produce as output a numeric list representing in a different way this text. It is important to note that the list of words extracted in previous section are transformed into texts, where each word is separated by a space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Given a vocabulary from a set of texts, i.e., all the words that appear in these texts, each text $i$ is represented by a vector $\\vec{d_{i}} = (w_{i,1},w_{i,2},...,w_{i,t})$, where $w_{i,j}$ represents the TF-IDF (Term Frequency - Inverse Document Frequency) from the $j$-th word of the vocabulary. The TF-IDF is as statistic measure used to evaluate how important a word is to a text in relation to a collection of texts [Manning et al. 2008a]. This measure is obtained through the multiplication of two terms:\n",
    "\n",
    "* **Term Frequency (TF):** measure how often a word $t$ occurs in a text through the equation $ TF(t) = \\dfrac{\\text{Number of times that } t \\text{ appears in the text}}{\\text{Total number of words in the text}} $.\n",
    "\n",
    "* **Inverse Document Frequency (IDF):** measure how important a word $t$ is through the equation $IDF(t) = \\log_e{\\left(\\frac{\\text{Total number of texts}}{\\text{Number of texts with the word } t}\\right)}$.\n",
    "\n",
    "Briefly, each text is represented by a sparse array that contains their TF-IDF values for each word in the vocabulary. The vocabulary can be reduced to a number $V$ of words, the most prevalent in the set of texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Brazilian Malware dataset, below we present a way to extract features from the list of libraries used by each sample using TF-IDF, through the class [*TfidfVectorizer*](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) from scikit-learn [Pedregosa et al. 2011]. First, the dataset is read, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# dataset location\n",
    "data_path = \"./datasets/brazilian-malware.csv\"\n",
    "# read CSV dataset\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the extractor is initialized with a maximum of 200 features (each feature represents a word) and the set of samples is divided into two: the first set is used to train the extractor (using the method *fit*, which generates the vocabulary used) and the later, to test. Finally, both sets have their features extracted through the method *transform*. Looking at the output, we can see that the resulting matrix has 200 columns, the maximum number of features we selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25091, 200) (25090, 200)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# get texts\n",
    "texts = data[\"ImportedDlls\"].values\n",
    "# get mid of data\n",
    "mid = int((len(texts) + 1)/2)\n",
    "# split data into train and test\n",
    "train_texts = texts[:mid]\n",
    "test_texts = texts[mid:]\n",
    "# initialize vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=200)\n",
    "# train vectorizer\n",
    "vectorizer.fit(train_texts)\n",
    "# transform train texts to features\n",
    "train_features = vectorizer.transform(train_texts)\n",
    "test_features = vectorizer.transform(test_texts)\n",
    "# print features shape\n",
    "print(train_features.shape, test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "Developed in 2013 by Google researchers [Mikolov et al. 2013a], word2vec is a way to create vectors that represent words. The greatest contribution of this method is its capability of representing the similarity between words and their meanings, different from TF-IDF, which just project data based on their frequency. Besides that, word2vec projects only words and not the text, making necessary to use a strategy to extract features from the projected words. The word2vec general model uses a feed-forward non-supervised neural network (for more details about neural networks, check [section 5](05_models.ipynb)). There are two architectures available for this model: continuous bag-of-words (CBOW) e skip-gram. The first one (CBOW) tries to predict the central word from a set of words around them (input), similar to a cluster. In contrast, the skip-gram architecture does the opposite: it tries to predict the words around, given a central word as input. The figure below presents both methods [Shulman 2016, Mikolov et al. 2013b]. CBOW was the architecture used in this work and will be explained in details further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"imgs/word2vec1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, each input of the network is codified into an vector $x_{ck} = \\{x_{1}, x_{2}, ..., x_{V}\\}$, where $V$ is the vocabulary size, $c$ is the word identifier and $k$, the text identifier (as presented in the figure below). The item $x_{i}$ is 1 only if $i$ corresponds to the word identifier $c$. Otherwise, $x_{i}$ is 0, i.e., only one position of the array of each word will have value 1 (only the one that corresponds to the word). This codification is called one-hot encoding. Through this input, the model calculates the average of these arrays and multiplies it by the weights from the hidden layer $h$, generating the output $y$. More details about the training of this network can be found in [Rong 2014]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"imgs/word2vec4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code bellow presents a way to extract features using word2vec. For that, we implemented a class called *MeanEmbeddingVectorizer*, which uses the model [*Word2Vec*](https://radimrehurek.com/gensim/models/word2vec.html) implemented by the library gensim [Rehurek and Sojka 2010]. In this class, we defined only two parameters (more parameters can be added when modifying this class: *size*, which defines the dimension used to project the words and *min_count*, which defines the minimum of apparition for a word to be considered. In the method *fit* we train the *Word2Vec* model with the train data and obtain the projection of these data. With the method *transform*, the data passed as parameter use the created projection to create a feature vector for each sample, based on the average of their words (in case the word does not exist, it is considered as being a null vector). Thus, the class that we implemented works very similar to the one implemented by scikit-learn's TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "# vectorizer class: calc average of words using word2vec\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, size, min_count=1): #word2vec):\n",
    "        self.size = size\n",
    "        self.min_count = 1\n",
    "\n",
    "    def fit(self, X):\n",
    "        w2v = Word2Vec(X,size=self.size,min_count=self.min_count)\n",
    "        self.word2vec = dict(zip(w2v.wv.index2word,w2v.wv.vectors))\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(list(self.word2vec.values())[0])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25091, 200) (25090, 200)\n"
     ]
    }
   ],
   "source": [
    "# get texts\n",
    "texts = data[\"ImportedDlls\"].values\n",
    "# get mid of data\n",
    "mid = int((len(texts) + 1)/2)\n",
    "# split data into train and test\n",
    "train_texts = texts[:mid]\n",
    "test_texts = texts[mid:]\n",
    "# initialize vectorizer\n",
    "m = MeanEmbeddingVectorizer(size=200)\n",
    "# train vectorizer\n",
    "m.fit(train_texts)\n",
    "# transform train and test texts to w2v mean\n",
    "train_features = m.transform(train_texts)\n",
    "test_features = m.transform(test_texts)\n",
    "# print features shape\n",
    "print(train_features.shape, test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "One important step right after feature extraction is the normalization, given that many machine learning algorithms cannot have a good performance when the features have different scales [Gron 2017]. Some algorithms can give more relevance to higher values, even when they represent totally different things. For example, given two features height, in meters, and weight, in kilos, to classify how healthy a person is, the difference of scale between these two features could give a bigger significance to weight, given that it has more variation in scale than the height. Usually two types of normalization are used [Gron 2017]: min-max scaling and standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-Max Scaling\n",
    "\n",
    "The min-max rescales all the features to an interval between zero and one, using the equation $z_{i} = \\frac{x_{i}-min(x)}{max(x)-min(x)}$, where $x_{i}$ is the value of a feature $x$ of the sample $i$, $min(x)$ is the lowest value from this feature in the training set and $max(x)$, the biggest value of this feature in the same set [Gron 2017]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library scikit-learn [Pedregosa et al. 2011] has a class called [*MinMaxScaler*](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html), which implements this normalization, as shown below. First, min-max is initialized, specifying the interval that the features must be normalized (which is an advantage of this method, given that some algorithms work better with certain intervals, such as neural networks - 0 to 1 - and SVM with RBF kernel - -1 to 1). Thus, this technique uses the train set to extract the lowest and the biggest values of each feature (through the method *fit*) to further normalize them (though the method *transform*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25091, 200) (25090, 200)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# initialize minmax\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# train minmax\n",
    "scaler.fit(train_features)\n",
    "# transform features\n",
    "train_features_norm = scaler.transform(train_features)\n",
    "test_features_norm = scaler.transform(test_features)\n",
    "print(train_features_norm.shape, test_features_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "\n",
    "The standardization technique normalizes all the data subtracting from each feature its average value (based on the train data) and dividing this value by the variance, in a way that the resulting distribution has unitary variance, as shown by the equation $z_{i} = \\frac{x_{i} - u}{s}$, where $x_{i}$ is the value of the feature $x$ from the sample $i$, $u$ is the average of the training samples from this feature and $s$, the variance of the train data, also from this feature. One advantage of this method is that it is not so affected by outliers as min-max, given that if there is any outlier in the training data, min-max will consider this value, creating a great interval between the \"normal\" data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to min-max, standardization already has an implementation in scikit-learn [Pedregosa et al. 2011] through the class [*StandardScaler*](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), as shown in the code below. You just need to initialize it, train with the training set to get the average and variance of each feature (through the method *fit*) and, finally, normalize the data (through the method *transform*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25091, 200) (25090, 200)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# initialize minmax\n",
    "scaler = StandardScaler()\n",
    "# train minmax\n",
    "scaler.fit(train_features)\n",
    "# transform features\n",
    "train_features_norm = scaler.transform(train_features)\n",
    "test_features_norm = scaler.transform(test_features)\n",
    "print(train_features_norm.shape, test_features_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Gron 2017] Gron, A. (2017). Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O’Reilly Media, Inc., 1st edition.\n",
    "\n",
    "[Manning et al. 2008a] Manning, C. D., Raghavan, P., and Schütze, H. (2008a). Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA.\n",
    "\n",
    "[Mikolov et al. 2013a] Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013a). Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.\n",
    "\n",
    "[Pedregosa et al. 2011] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. (2011). Scikitlearn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.\n",
    "\n",
    "[Rehurek and Sojka 2010] Reh ˚u ˇ ˇrek, R. and Sojka, P. (2010). Software Framework for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45–50, Valletta, Malta. ELRA. http: //is.muni.cz/publication/884893/en.\n",
    "\n",
    "[Rong 2014] Rong, X. (2014). word2vec parameter learning explained. CoRR, abs/1411.2738."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**<< Previous Section**](03_attributes.ipynb) | [**Next Section >>**](05_models.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
