{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Feature Extraction\n",
    "\n",
    "Sometimes the attributes extracted cannot be used directly in a machine learning model. From the attributes presented in previous section, for example, only the numeric attributes (which values are real or integer) could be used directly in a model. The list of words extracted, such as libraries and system calls used by a software, must pass through one more step called feature extraction, whose objective is to transform these attributes into something readable by the classifier, usually defining the same number of features as output of this process (once a program can use more libraries than others, for example). In this section we introduce how to extract features from textual attributes (using TF-IDF, Word2Vec and Encoders) and how to normalize them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "We are going to present two textual feature extraction methods, both of them used a lot in the literature: TF-IDF and Word2Vec. Given a text representing an attribute, both methods produce as output a numeric list representing in a different way this text. It is important to note that the list of words extracted in previous section are transformed into texts, where each word is separated by a space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-DF\n",
    "\n",
    "Given a vocabulary from a set of documents, i.e., all the words that appear in these documents, each document ![i](imgs/tfidf1.png) is represented by a vector ![vec](imgs/tfidf2.png), where ![wij](imgs/tfidf3.png) represents the TF-IDF (Term Frequency - Inverse Document Frequency) from the j-th word of the vocabulary. The TF-IDF is as statistic measure used to evaluate how important a word is to a document in relation to a collection of documents [Manning et al. 2008a]. This measure is obtained through the multiplication of two terms:\n",
    "\n",
    "* **Term Frequency (TF):** measure how often a word ![t](imgs/tfidf4.png) occurs in a document through the equation ![tf](imgs/tfidf5.png).\n",
    "\n",
    "* **Inverse Document Frequency (IDF):** measure how important a word ![t](imgs/tfidf4.png) is through the equation ![idf](imgs/tfidf6.png)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eq1](/imgs/tfidf1.png]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# dataset location\n",
    "data_path = \"./datasets/brazilian-malware.csv\"\n",
    "# read CSV dataset\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert DLLs to features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25091, 200) (25090, 200)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# get texts\n",
    "texts = data[\"ImportedDlls\"].values\n",
    "# get mid of data\n",
    "mid = int((len(texts) + 1)/2)\n",
    "# split data into train and test\n",
    "train_texts = texts[:mid]\n",
    "test_texts = texts[mid:]\n",
    "# initialize vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=200)\n",
    "# train vectorizer\n",
    "vectorizer.fit(train_texts)\n",
    "# transform train texts to features\n",
    "train_features = vectorizer.transform(train_texts)\n",
    "test_features = vectorizer.transform(test_texts)\n",
    "# print features shape\n",
    "print(train_features.shape, test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25091, 200) (25090, 200)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# vectorizer class: calc average of words using word2vec\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, size, min_count=1): #word2vec):\n",
    "        self.size = size\n",
    "        self.min_count = 1\n",
    "\n",
    "    def fit(self, X):\n",
    "        w2v = Word2Vec(X,size=self.size,min_count=self.min_count)\n",
    "        self.word2vec = dict(zip(w2v.wv.index2word,w2v.wv.vectors))\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(list(self.word2vec.values())[0])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "# get texts\n",
    "texts = data[\"ImportedDlls\"].values\n",
    "# get mid of data\n",
    "mid = int((len(texts) + 1)/2)\n",
    "# split data into train and test\n",
    "train_texts = texts[:mid]\n",
    "test_texts = texts[mid:]\n",
    "# initialize vectorizer\n",
    "m = MeanEmbeddingVectorizer(size=200)\n",
    "# train vectorizer\n",
    "m.fit(train_texts)\n",
    "# transform train and test texts to w2v mean\n",
    "train_features = m.transform(train_texts)\n",
    "test_features = m.transform(test_texts)\n",
    "# print features shape\n",
    "print(train_features.shape, test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-Max\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25091, 200) (25090, 200)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# initialize minmax\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# train minmax\n",
    "scaler.fit(train_features.toarray())\n",
    "# transform features\n",
    "train_features_norm = scaler.transform(train_features.toarray())\n",
    "test_features_norm = scaler.transform(test_features.toarray())\n",
    "print(train_features_norm.shape, test_features_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# initialize minmax\n",
    "scaler = StandardScaler()\n",
    "# train minmax\n",
    "scaler.fit(train_features.toarray())\n",
    "# transform features\n",
    "train_features_norm = scaler.transform(train_features.toarray())\n",
    "test_features_norm = scaler.transform(test_features.toarray())\n",
    "print(train_features_norm.shape, test_features_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
